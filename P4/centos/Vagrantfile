# -*- mode: ruby -*-
# vi: set ft=ruby :
#instalar hostmanager plugin
#
DIRCPD = '.'

disk1 = DIRCPD + "/disk1.vdi"
disk2 = DIRCPD + "/disk2.vdi"
disk3 = DIRCPD + "/disk3.vdi"

Vagrant.configure(2) do |config|
  config.vm.box = "centos/7"
  config.vm.provision "shell", inline: <<-SHELL
        sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
        systemctl restart sshd.service
        yum update
        yum -y install epel-release net-tools
	# añado estas lineas para instalar gluster en etapa de provisioning - JTerron
	yum -y install centos-release-gluster7
	yum -y install glusterfs glusterfs-cli glusterfs-libs glusterfs-server
	systemctl enable glusterd.service
	systemctl start glusterd.service

        echo "192.168.12.11 centos1" >> /etc/hosts
        echo "192.168.12.12 centos2" >> /etc/hosts
        echo "192.168.12.13 centos3" >> /etc/hosts
  SHELL
  config.vm.define :centos1 do |centos1_config|
      centos1_config.vm.hostname = "centos1.vm"
      centos1_config.vm.network "private_network" , ip:"192.168.12.11"
      centos1_config.vm.synced_folder ".","/vagrant"
      centos1_config.vm.provider :virtualbox do |vb|
          vb.name = "centos1"
          vb.customize ["modifyvm", :id, "--memory", "768"]
          vb.customize ["modifyvm", :id, "--cpus", "1"]
          unless File.exist?(disk1)
            vb.customize ['createhd', '--filename', disk1, '--size', 10 * 1024]
            vb.customize ['storageattach', :id, '--storagectl', 'IDE', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', disk1]
          end
      end
      centos1_config.vm.provision "shell", inline: <<-SHELL
        # Ahora los bricks
        (echo n; echo p; echo 1; echo 2048; echo 20971519; echo t; echo 8e; echo w;) | fdisk /dev/sdb
        partprobe
        pvcreate /dev/sdb1
        vgcreate vg01 /dev/sdb1
        lvcreate -l 100%FREE -n lv01 vg01
        mkfs.xfs /dev/mapper//vg01-lv01
        mkdir -p /gluster/bricks/brick1
        echo "/dev/mapper/vg01-lv01 /gluster/bricks/brick1 xfs defaults 0 0" >> /etc/fstab
        mount -a
        mkdir /gluster/bricks/brick1/vol1
        
      SHELL
  end

  config.vm.define :centos2 do |centos2_config|
      centos2_config.vm.hostname = "centos2.vm"
      centos2_config.vm.network "private_network" , ip:"192.168.12.12"
      centos2_config.vm.synced_folder ".","/vagrant"
      centos2_config.vm.provider :virtualbox do |vb|
          vb.name = "centos2"
          vb.customize ["modifyvm", :id, "--memory", "768"]
          vb.customize ["modifyvm", :id, "--cpus", "1"]
          unless File.exist?(disk2)
            vb.customize ['createhd', '--filename', disk2, '--size', 10 * 1024]
            vb.customize ['storageattach', :id, '--storagectl', 'IDE', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', disk2]
          end
      end
      # https://www.vagrantup.com/docs/provisioning/basic_usage#multiple-provisioners
      centos2_config.vm.provision "shell", inline: <<-SHELL
      # Ahora los bricks
      (echo n; echo p; echo 1; echo 2048; echo 20971519; echo t; echo 8e; echo w) | fdisk /dev/sdb
      partprobe
      pvcreate /dev/sdb1
      vgcreate vg01 /dev/sdb1
      lvcreate -l 100%FREE -n lv01 vg01
      mkfs.xfs /dev/mapper//vg01-lv01
      mkdir -p /gluster/bricks/brick1
      echo "/dev/mapper/vg01-lv01 /gluster/bricks/brick1 xfs defaults 0 0" >> /etc/fstab
      mount -a
      mkdir /gluster/bricks/brick1/vol1
      SHELL
  end

config.vm.define :centos3 do |centos3_config|
      centos3_config.vm.hostname = "centos3.vm"
      centos3_config.vm.network "private_network" , ip:"192.168.12.13"
      centos3_config.vm.synced_folder ".","/vagrant"
      centos3_config.vm.provider :virtualbox do |vb|
          vb.name = "centos3"
          vb.customize ["modifyvm", :id, "--memory", "768"]
          vb.customize ["modifyvm", :id, "--cpus", "1"]
          unless File.exist?(disk3)
            vb.customize ['createhd', '--filename', disk3, '--size', 10 * 1024]
            vb.customize ['storageattach', :id, '--storagectl', 'IDE', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', disk3]
          end
          # Para comprobar el estado del cluster visto desde peer3
      centos3_config.vm.provision "shell", inline: <<-SHELL
        gluster peer status
        # Esto se ejecuta en cualquiera de las máquinas y crea el volumen:
        gluster volume create glustervol1 replica 2 transport tcp centos1:/gluster/bricks/brick1/vol1 centos2:/gluster/bricks/brick1/vol1
        gluster volume start glustervol1
        gluster volume info glustervol1
        mount -t glusterfs centos1:/glustervol1 /gdatos1
      SHELL
      end
      
  end

end
